{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Scenario MORDM\n",
    "\n",
    "Multi-scenario MORMD is an extension of normal MORDM to better include robustness considerations within the search phase. It starts from the scenario discovery results resulting from MORDM. Next, from the experiments within this box, a set of scenarios is selected. \n",
    "\n",
    "There are many ways of selecting the additional scenarios. The original paper which introduced multi-scenario MORMD [Watson and Kaspzryk (2017)](https://doi.org/10.1016/j.envsoft.2016.12.001) did it in a more or less adhoc manner. [Eker and Kwakkel (2018)](https://doi.org/10.1016/j.envsoft.2018.03.029) introduced a more formal selection approach, the code of which can be found on [GitHub](https://github.com/sibeleker/MORDM---Multi-scenario-search). \n",
    "\n",
    "For this assignment, make an informed selection of 4 scenarios, using an approach of your choice. Motivate carefully your selection procedure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import load_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ba825bba7562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutcomes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./results/selected_results.tar.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_results' is not defined"
     ]
    }
   ],
   "source": [
    "results, outcomes = load_results('./results/selected_results.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "lake_model = Model('lakemodel', function=lake_problem)\n",
    "lake_model.time_horizon = 100\n",
    "\n",
    "lake_model.uncertainties = [RealParameter(\"mean\", 0.01, 0.05),\n",
    "                            RealParameter(\"stdev\", 0.001, 0.005),\n",
    "                            RealParameter(\"b\", 0.1, 0.45),\n",
    "                            RealParameter(\"q\", 2, 4.5),\n",
    "                            RealParameter(\"delta\", 0.93, 0.99)]\n",
    "\n",
    "lake_model.levers = [RealParameter(\"c1\", -2, 2),\n",
    "                     RealParameter(\"c2\", -2, 2),\n",
    "                     RealParameter(\"r1\", 0, 2),\n",
    "                     RealParameter(\"r2\", 0, 2),\n",
    "                     RealParameter(\"w1\", 0, 1)]\n",
    "\n",
    "lake_model.outcomes = [ScalarOutcome(\"max_P\", kind=ScalarOutcome.MINIMIZE, expected_range=(0, 5)),\n",
    "                       ScalarOutcome(\"utility\", kind=ScalarOutcome.MAXIMIZE, expected_range=(0, 2)),\n",
    "                       ScalarOutcome(\"inertia\", kind=ScalarOutcome.MAXIMIZE, expected_range=(0, 1)),\n",
    "                       ScalarOutcome(\"reliability\", kind=ScalarOutcome.MAXIMIZE, expected_range=(0, 1))]\n",
    "\n",
    "lake_model.constants = [Constant(\"alpha\", 0.4),\n",
    "                        Constant(\"nsamples\", 100),\n",
    "                        Constant(\"myears\", 100)]\n",
    "\n",
    "n_scenarios = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario Selection\n",
    "The way in which the scenario will be selected will be by arbitrarily choosing the scenario with the highest utility in the set, the lowest and two closest to the median value of utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes = pd.DataFrame(outcomes)\n",
    "median = df_outcomes['utility'].median()\n",
    "\n",
    "df_median = df_outcomes.iloc[((df_outcomes['utility']-median).abs().argsort()[:2])]\n",
    "df_outcomes = df_outcomes.loc[(df_outcomes['utility'] == max(df_outcomes['utility'])) |\n",
    "                              (df_outcomes['utility'] == min(df_outcomes['utility']))]\n",
    "df_outcomes = pd.concat([df_outcomes, df_median])\n",
    "index_list = df_outcomes.index.tolist()\n",
    "results = results.iloc[index_list].reset_index(drop=True)\n",
    "df_outcomes.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for each scenario\n",
    "\n",
    "For each of the four selected scenarios, use many-objective optimization to find a pareto approximate set using the same approach as for assignment 8. Remember to check for convergence (and time permitting, seed analysis), and be careful in what epsilon values to use (not to coarse, not too small). \n",
    "\n",
    "Store the resulting set of pareto solutions in a smart way for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_metrics = [HyperVolume.from_outcomes(lake_model.outcomes),\n",
    "                           EpsilonProgress()]\n",
    "\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results_, convergence = evaluator.optimize(nfe=10000, searchover='levers',\n",
    "                                               epsilons=[0.1, 0.1, 0.05, 0.05],\n",
    "                                               convergence=convergence_metrics)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = results.loc[:, ['max_P', 'utility', \"inertia\", 'reliability']]\n",
    "limits = parcoords.get_limits(outcomes)\n",
    "axes = parcoords.ParallelAxes(limits)\n",
    "axes.plot(outcomes)\n",
    "\n",
    "# we invert this axis so direction of desirability is the same\n",
    "axes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots generated by these models, it is clear that hypervolume stabilized around 4,000 function evaluations. However, the epsilon progress has stabilized just before 10,000 and the nfe will therefore be kept at 10,000, as any increase in this will simply be a waste of resources and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-evaluate under deep uncertainty\n",
    "\n",
    "Combine the pareto set of solutions found for each scenario. Next, turn each solution into a policy object. If you have a very large number of policies, you can choose to down sample your policies in some reasoned way (*e.g.*, picking min and max on each objective, slicing across the pareto front with a particular step size). As a rule of thumb, try to limit the set of policies to at most 50. \n",
    "\n",
    "Re-evaluate the combined set of solutions over 1000 scenarios sampled using LHS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_results = new_results.drop(['max_P', 'utility', \"inertia\", 'reliability'], axis=1)\n",
    "new_results_dict = new_results.to_dict('index')\n",
    "policies = []\n",
    "for dict_index in range(len(new_results_dict)):\n",
    "    policy_dict = new_results_dict[dict_index]\n",
    "    policies.append(Policy(f\"pol_{dict_index}\", **policy_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run the model multiple times, the number of policies has never exceeded 50 and therefore, all the policies will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenarios = 1000\n",
    "LHS = 'lhs'\n",
    "with MultiprocessingEvaluator(lake_model) as evaluator:\n",
    "    results__, outcomes__ = evaluator.perform_experiments(scenarios=n_scenarios,\n",
    "                                                          policies=policies,\n",
    "                                                          uncertainty_sampling=LHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate both the maximum regret, and the domain criterion using the values provided in [Bartholomew and Kwakkel (2020)](https://doi.org/10.1016/j.envsoft.2020.104699). Ignore the max_P objective.\n",
    "\n",
    "visualize the results in parallel coordinate plot. \n",
    "\n",
    "Are there any promising compromise solutions which balance performance in both the reference scenarios as well as in terms of their robustness?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regret(outcomes_w_policy, results_w_policy) :\n",
    "    df_outcomes_w_policy = pd.DataFrame(outcomes_w_policy)\n",
    "\n",
    "    results_w_policy.rename(columns={\"max_P\": \"max_P_initial\",\n",
    "                                     \"inertia\": \"inertia_initial\",\n",
    "                                     \"utility\": \"utility_initial\",\n",
    "                                     \"reliability\": \"reliability_initial\"},\n",
    "                            inplace=True)\n",
    "    df_full = pd.concat([df_outcomes_w_policy, results_w_policy], axis=1)\n",
    "    df_regret = pd.DataFrame(columns=['policy_num', 'max_P', 'utility', \"inertia\", 'reliability'])\n",
    "    for unique_item in df_full['policy'].unique() :\n",
    "        df_partial = df_full.loc[df_full['policy'] == unique_item]\n",
    "        df_partial.reset_index(drop=True, inplace=True)\n",
    "        max_max_P = max(df_partial['max_P'])\n",
    "        max_utility = max(df_partial['utility'])\n",
    "        max_inertia = max(df_partial['inertia'])\n",
    "        max_reliability = max(df_partial['reliability'])\n",
    "        for index in range(len(df_partial)) :\n",
    "            loc_max_p = df_partial.loc[index, 'max_P']\n",
    "            loc_utility = df_partial.loc[index, 'utility']\n",
    "            loc_inertia = df_partial.loc[index, 'inertia']\n",
    "            loc_reliability = df_partial.loc[index, 'reliability']\n",
    "            new_row = {'policy_num' : unique_item,\n",
    "                       'max_P' : max_max_P - loc_max_p,\n",
    "                       'utility' : max_utility - loc_utility,\n",
    "                       'inertia' : max_inertia - loc_inertia,\n",
    "                       'reliability' : max_reliability - loc_reliability}\n",
    "            df_regret = df_regret.append(new_row, ignore_index=True)\n",
    "\n",
    "    columns_to_sum = ['max_P', 'utility', \"inertia\", 'reliability']\n",
    "    df_regret['total_regret'] = df_regret[columns_to_sum].sum(axis=1)\n",
    "\n",
    "    return df_regret\n",
    "df_regret = calculate_regret(outcomes__, results__)\n",
    "\n",
    "df_regret_plot = pd.DataFrame(columns=['policy', 'max_regret', 'min_regret'])\n",
    "for unique_item in df_regret['policy_num'].unique() :\n",
    "    df_partial = df_regret.loc[df_regret['policy_num'] == unique_item]\n",
    "    new_row = {'policy' : unique_item,\n",
    "               'max_regret' : max(df_partial['total_regret']),\n",
    "               'min_regret' : min(df_partial['total_regret'])}\n",
    "    df_regret_plot = df_regret_plot.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regret_plot.plot(y='max_regret', style='o', use_index=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the runs conducted within the program, the best solutions that have been found have c1 and c2 values that are positive around 0.5-0.7, r1 and r2 which are highly positive 1.5-1.7 and a very low w1 at around 0.05-0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
