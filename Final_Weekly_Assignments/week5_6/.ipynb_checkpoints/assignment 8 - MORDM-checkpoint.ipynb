{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-objective robust decision making (MORDM)\n",
    "\n",
    "\n",
    "This exercise demostrates the application of MORDM on the lake model, which was used in earlier exercises.\n",
    "\n",
    "MORDM has four main steps:\n",
    "\n",
    "(i)\t    **problem formulation** based on a systems analytical problem definition framework \n",
    "\n",
    "(ii)\t**searching** for candidate solutions that optimize multiple objectives by using multi-objective evolutionary algorithms \n",
    "\n",
    "(iii)\tgenerating an ensemble of scenarios to **explore** the effects of uncertainties \n",
    "\n",
    "(iv)\tusing **scenario discovery** to detect the vulnerabilities of candidate solutions and improving thecandidate solutions\n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Problem formulation\n",
    "### Lake Model\n",
    "\n",
    "Remember the lake problem used in the assignments in previous weeks. The lake problem is a hypothetical case where the inhabitants of a lake town decide on the amount of annual pollution they release into a lake. It the pollution in the lake passes a threshold, it will suffer irreversible eutrophication.\n",
    "\n",
    "The lake problem has 4 **outcome indicators**: \n",
    "   - **max_P**: maximum pollution over time, to be minimized\n",
    "   - **utility**: economic benefits obtained from polluting the lake, to be maximized\n",
    "   - **inertia**: the percentage of significant annual changes in the anthropogenic pollution rate, to be maximized\n",
    "   - **reliability**: the percentage of years where the pollution level is below the critical threshold, to be maximized\n",
    "    \n",
    "See the lake model exercise for the formulation of these outcome variables.\n",
    "\n",
    "The lake problem is characterized by both stochastic uncertainty and **deep uncertainty**. The stochastic uncertainty arises from the natural inflow. To reduce this stochastic uncertainty, multiple replications are performed and the average over the replication is taken. Deep uncertainty is presented by uncertainty about the mean $\\mu$ and standard deviation $sigma$ of the lognormal distribution characterizing the natural inflow, the natural removal rate of the lake $\\beta$, the natural recycling rate of the lake $q$, and the discount rate $\\delta$. The table below specifies the ranges for the deeply uncertain factors, as well as their best estimate or default values. \n",
    "\n",
    "|Parameter\t|Range\t        |Default value|\n",
    "|-----------|--------------:|------------:|\n",
    "|$\\mu$    \t|0.01 – 0.05\t|0.02         |\n",
    "|$\\sigma$\t|0.001 – 0.005 \t|0.0017       |\n",
    "|$b$      \t|0.1 – 0.45\t    |0.42         |\n",
    "|$q$\t    |2 – 4.5\t    |2            |\n",
    "|$\\delta$\t|0.93 – 0.99\t|0.98         |\n",
    "\n",
    "\n",
    "The lake problem in previous assignments had 100 decision **levers**, meaning that the decision makers independently decide on the amount of anthropogenic pollution at every time step (100). Then a 'policy' was a set of values for these 100 levers, which you composed by sampling from the range [0, 0.1].   \n",
    "\n",
    "In this exercise, we will use a more advanced way of deciding on the amout of anhtropogenic polution. We will use a **closed loop** version of the lake model, meaning that $a_t$ (anthropogenic pollution) is dependent on $X_t$ (the pollution level at time t). For instance, the rate of anthropogenic pollutions is lowered if the pollution level is approaching a critical threshold. Here, we use \"cubic radial basis functions\" following [Quinn et al. 2017](http://www.sciencedirect.com/science/article/pii/S1364815216302250) and formulate $a_t$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{t} =  min\\Bigg(max\\bigg(\\sum\\limits_{j=1}^{n} w_{j}\\left\\vert{\\frac{X_{t,i}-c_{j}}{r_{j}}}\\right\\vert^3, 0.01\\bigg), 0.1\\Bigg) \\\\\n",
    "    s.t. \\\\\n",
    "    -2 \\leq c_{j} \\leq 2 \\\\\n",
    "    0 \\leq r_{j} \\leq 2 \\\\ \n",
    "    0 \\leq w_{j} \\leq 1 \\\\\n",
    "    \\sum\\limits_{j=1}^{n} w_{j} = 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The parameters that define this function also define the pollution strategy over time. Hence, the decision **levers** are the five parameters of this functions, namely $c_1$, $c_2$, $r_1$, $r_2$ and $w_1$. ($w_2$ = 1 - $w_1$).\n",
    "\n",
    "Note:: i is index for the realization, given m realizations; j is the index for the radial basis function, given 2 radial basis functions. \n",
    "\n",
    "**To formulate this problem, do the following:**\n",
    "\n",
    "**1) Import the lake model function from dps_lake_model.py**\n",
    "\n",
    "**2) Create an ema_workbench interface for this problem, with corresponding uncertainties, levers and outcomes as specified above**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "EMAError",
     "evalue": "name of model should only contain alpha numerical characters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEMAError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c35c3f9cd8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlake_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lake model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlake_problem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlake_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_horizon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ema_workbench/em_framework/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, function)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/ema_workbench/em_framework/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             raise EMAError((\"name of model should only contain \"\n\u001b[0m\u001b[1;32m     96\u001b[0m                             \"alpha numerical characters\"))\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEMAError\u001b[0m: name of model should only contain alpha numerical characters"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from Week_5_6_quaq.dps_lake_model import lake_model as lake_problem\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, ScalarOutcome, Constant, SequentialEvaluator, MultiprocessingEvaluator,\n",
    "                           ema_logging, Constraint, Policy, save_results)\n",
    "from ema_workbench.analysis import (parcoords, prim)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "lake_model = Model('lake_model', function=lake_problem)\n",
    "lake_model.time_horizon = 100\n",
    "\n",
    "lake_model.uncertainties = [RealParameter(\"mean\", 0.01, 0.05),\n",
    "                            RealParameter(\"stdev\", 0.001, 0.005),\n",
    "                            RealParameter(\"b\", 0.1, 0.45),\n",
    "                            RealParameter(\"q\", 2, 4.5),\n",
    "                            RealParameter(\"delta\", 0.93, 0.99)]\n",
    "\n",
    "lake_model.levers = [RealParameter(\"c1\", -2, 2),\n",
    "                     RealParameter(\"c2\", -2, 2),\n",
    "                     RealParameter(\"r1\", 0, 2),\n",
    "                     RealParameter(\"r2\", 0, 2),\n",
    "                     RealParameter(\"w1\", 0, 1)]\n",
    "\n",
    "lake_model.outcomes = [ScalarOutcome(\"max_P\", kind=ScalarOutcome.MINIMIZE),\n",
    "                       ScalarOutcome(\"utility\", kind=ScalarOutcome.MAXIMIZE),\n",
    "                       ScalarOutcome(\"inertia\", kind=ScalarOutcome.MAXIMIZE),\n",
    "                       ScalarOutcome(\"reliability\", kind=ScalarOutcome.MAXIMIZE)]\n",
    "\n",
    "n_scenarios = 100\n",
    "\n",
    "with SequentialEvaluator(lake_model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Searching for candidate solutions\n",
    "\n",
    "In the second step of MORDM, candidate strategies are identified which are pareto optimal conditional on a reference scenario. These candiate strategies are identified through search with multi-objective evolutionary algorithms, that iteratively evaluate a large number of alternatives on multiple objectives until they find the best candidates. For instance, when we optimize the lake model levers, the lake model function will be called for each candidate evaluation, and the corresponding four objective values will be generated. \n",
    "\n",
    "Take the model interface developed in the previous step and use the optimization functionality of the workbench to identify the pareto approximate set of solutions. Try the following:\n",
    "* change the epsilon values between 0.01 and 0.1, what changes, why?\n",
    "* change the number of function evaluations from 1000 to 10.000 (this requires using multiprocessing unless you are very patient). What is the difference? You can use  convergence as explained in assignment 7 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SequentialEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-77bce0b1bd2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mSequentialEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlake_model\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnfe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnfe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutcomes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'max_P'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utility'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reliability'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SequentialEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "nfes = [1000, 10000]\n",
    "epsilons = [0.01, 0.02, 0.05, 0.1]\n",
    "for nfe in nfes:\n",
    "    for epsilon in epsilons:\n",
    "        epsilon = [epsilon, epsilon, epsilon, epsilon]\n",
    "        with SequentialEvaluator(lake_model) as evaluator:\n",
    "            results = evaluator.optimize(epsilons=epsilon, nfe=nfe)\n",
    "        outcomes = results.loc[:, ['max_P', 'utility', \"inertia\", 'reliability']]\n",
    "        limits = analysis.parcoords.get_limits(outcomes)\n",
    "        axes = analysis.parcoords.ParallelAxes(limits)\n",
    "        axes.plot(outcomes)\n",
    "\n",
    "        # we invert this axis so direction of desirability is the same \n",
    "        axes.invert_axis('max_P') \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different epsilons and nfe's\n",
    "The different epsilon values give different sets of optimal solution. The higher the epsilon values, the less final solutions are given. However, it can be seen from the plots that these solutions are distributed in a similar manner. \n",
    "Increasing the nfe's on the other hand, does not necessarily increase or decrease the number of final optimal solution, but the plot does suggest that the solutions that have been found are distributed more equally across each of the three parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot the tradeoffs you have found using a parallel axis plot**\n",
    "\n",
    "We can visualize these tradeoffs on **parallel axis plots**. In these plots, each dimension is shown as a vertical axis. Each solution is represented by a line on this plot, which crosses the objective axes at the corresponsing value. You can use the [parcoords functionality](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/analysis/parcoords.html) for this that comes with the ema_workbench. Ensure that the direction of desirability is the same for the four objectives.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does this plot tell us about the tradeoffs and conflicting objectives?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Re-evaluate candidate solutions under uncertainty\n",
    "\n",
    "We now have a large number of candidate solutions (policies), we can re-evaluate them over the various deeply uncertain factors to assess their robustness against uncertainties.\n",
    "\n",
    "For this robustness evaluation, we need to explore the scenarios for each solution. It means that, if we would like to run for instance 1000 scenarios for each solution, we might have to execute a very large number of runs.\n",
    "\n",
    "Here, to simplify the case, let's suppose that decision makers have a hard constrain on *reliability*. No solution with less than 90% reliability is acceptable for them. Therefore, we can reduce the size of the solution set according to this constraint. \n",
    "\n",
    "**Apply this constraint of reliability on the results, and create a new dataframe named new_results**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New results\n",
    "There are two ways in which one can obtain the dataframe for the new results. One way is to simply run the old version and filter out only the rows where the reliability is higher than 90%. Another way is to apply a constraint through the `Constraints` function in the `ema_workbench`. Since the exercise did not ask to run the full model again with the constraints variable applied, the first methodology has been chosen. However, if one wanted to apply the second methodolgy, the constraint code would look like the following:\n",
    "```python\n",
    "constraints = [Constraint(\"reliability\", \n",
    "                          outcome_names=\"reliability\", \n",
    "                          function=lambda x : max(0.9, 1.8-x))]\n",
    "\n",
    "            with SequentialEvaluator(lake_model) as evaluator:\n",
    "                new_results = evaluator.optimize(epsilons=epsilon, \n",
    "                                                 nfe=nfe, \n",
    "                                                 constraints=constraints)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will yield a new dataframe called `new_results`, where the results will only include those where the reliability is higher than 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = results.loc[results[\"reliability\"] >= 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**From new_results, which is the reduced dataframe of candidate solutions, make a list of policies in a format that can be inputed to the *perform_experiments* function of the EMA workbench.**\n",
    "\n",
    "*hint: you need to transform each policy to a dict, and then use this dict as input for the Policy class that comes with the workbench*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "policy_results = new_results.drop(['max_P', 'utility', \"inertia\", 'reliability'], axis=1)\n",
    "new_results_dict = new_results.to_dict('index')\n",
    "policies = []\n",
    "for dict_index in range(len(new_results_dict)):\n",
    "    policy_dict = new_results_dict[dict_index]\n",
    "    policies.append(Policy(f\"pol_{dict_index}\", **policy_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform 1000 scenarios for each of the policy options. Depending on how many solutions are left after implementing the constraint, consider using multiprocessing or ipyparallel to speed up calculations.**\n",
    "\n",
    "If you want to use ipyparallel, don't forget to start ipcluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_scenarios = 1000\n",
    "with SequentialEvaluator(lake_model) as evaluator:\n",
    "    results_w_policy = evaluator.perform_experiments(n_scenarios, \n",
    "                                                     policies=policies)\n",
    "results_w_policy.rename(columns={\"max_P\": \"max_P_initial\",\n",
    "                                 \"inertia\": \"inertia_initial\",\n",
    "                                 \"utility\": \"utility_initial\",\n",
    "                                 \"reliability\": \"reliability_initial\"},\n",
    "                        inplace=True)\n",
    "\n",
    "df_outcomes_w_policy = pd.DataFrame(outcomes_w_policy)\n",
    "\n",
    "df_full = pd.concat([df_outcomes_w_policy, results_w_policy], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the **robustness** of each of the policy options based on these scenario results. We can calculate the robustness of a policy option in terms of its performance on an outcome indicator across the 1000 scenarios. In other words, we can identify how robust a policy is in terms of each outcome indicator, and investigate the robustness tradeoffs.  \n",
    "\n",
    "There are multiple metrics to quantify robustness. On of them is the *signal to noise ratio*, which is simply the mean of a dataset divided by its standard deviation. For instance, for an outcome indicator to be maximized, we prefer a high average value across the scenarios, and a low standard deviation, implying a narrow uncertaintiy range. Therefore, we want to maximize the signal-to-noise ratio. For an outcome indicator to be minimized, a lower mean and a lower standard deviation is preferred. Therefore the formulation is different.\n",
    "\n",
    "**Write a function to calculate the signal-to-noise ratio for both kinds of outcome indicators. Calculate the signal-to-noise ratios for each outcome and each policy option. Plot the tradeoffs on a parallel axis plot. Which solutions look like a good compromise policy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_outcomes_w_policy = pd.DataFrame(outcomes_w_policy)\n",
    "\n",
    "signal_noise_r = []\n",
    "for policy_analyzed in range(len(policies)):\n",
    "    df = (df_outcomes_w_policy.tail((len(policies) - policy_analyzed)*n_scenarios)).head(n_scenarios)\n",
    "    df.loc[\"mean\"] = df.mean()\n",
    "    df.loc[\"stdev\"] = df.std()\n",
    "    df.loc[\"ratio\"] = df.loc[\"mean\"] / df.loc[\"stdev\"]\n",
    "    partial_dict = df.loc[\"ratio\"].to_dict()\n",
    "    signal_noise_r.append(partial_dict)\n",
    "    \n",
    "df_plot_signals = pd.DataFrame(signal_noise_r)\n",
    "\n",
    "limits = parcoords.get_limits(df_plot_signals)\n",
    "axes = parcoords.ParallelAxes(limits)\n",
    "axes.plot(df_plot_signals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another robustness metric is **maximum regret**, calculated again for each policy and for each outcome indicator. *Regret* is defined for each policy under each scenario, as the difference between the performance of the policy in a specific scenario and the berformance of a no-regret (i.e. best possible result in that scenario) policy. The *maximum regret*  is then the maximum of such regret values across all scenarios. We of course favor policy options with low *maximum regret* values. \n",
    "\n",
    "**Write a function to calculate the maximum regret. Calculate the maximum regret values for each outcome and each policy option. Plot the tradeoffs on a parallel plot. Which solutions look like a good compromise policy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regret() :\n",
    "    df_regret = pd.DataFrame(columns=['policy_num', 'max_P', 'utility', \"inertia\", 'reliability'])\n",
    "    for unique_item in df_full['policy'].unique() :\n",
    "        df_partial = df_full.loc[df_full['policy'] == unique_item]\n",
    "        df_partial.reset_index(drop=True, inplace=True)\n",
    "        max_max_P = max(df_partial['max_P'])\n",
    "        max_utility = max(df_partial['utility'])\n",
    "        max_inertia = max(df_partial['inertia'])\n",
    "        max_reliability = max(df_partial['reliability'])\n",
    "        for index in range(len(df_partial)) :\n",
    "            new_row = {'policy_num' : unique_item,\n",
    "                       'max_P' : max_max_P - df_partial.loc[index, 'max_P'],\n",
    "                       'utility' : max_utility - df_partial.loc[index, 'utility'],\n",
    "                       'inertia' : max_inertia - df_partial.loc[index, 'inertia'],\n",
    "                       'reliability' : max_reliability - df_partial.loc[index, 'reliability']}\n",
    "            df_regret = df_regret.append(new_row, ignore_index=True)\n",
    "\n",
    "    columns_to_sum = ['max_P', 'utility', \"inertia\", 'reliability']\n",
    "    df_regret['total_regret'] = df_regret[columns_to_sum].sum(axis=1)\n",
    "\n",
    "    return df_regret\n",
    "df_regret = calculate_regret()\n",
    "\n",
    "df_regret_plot = pd.DataFrame(columns=['policy', 'max_regret', 'min_regret'])\n",
    "for unique_item in df_regret['policy_num'].unique() :\n",
    "    df_partial = df_regret.loc[df_regret['policy_num'] == unique_item]\n",
    "    new_row = {'policy' : unique_item,\n",
    "               'max_regret' : max(df_partial['total_regret']),\n",
    "               'min_regret' : min(df_partial['total_regret'])}\n",
    "    df_regret_plot = df_regret_plot.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regret_plot.sort_values(['max_regret'], inplace=True)\n",
    "best_policy = df_regret_plot.loc[0, 'policy']\n",
    "best_c1 = results_w_policy.loc[results_w_policy['policy'] == best_policy, \"c1\"].iloc[0]\n",
    "best_c2 = results_w_policy.loc[results_w_policy['policy'] == best_policy, \"c2\"].iloc[0]\n",
    "best_r1 = results_w_policy.loc[results_w_policy['policy'] == best_policy, \"r1\"].iloc[0]\n",
    "best_r2 = results_w_policy.loc[results_w_policy['policy'] == best_policy, \"r2\"].iloc[0]\n",
    "best_w1 = results_w_policy.loc[results_w_policy['policy'] == best_policy, \"w1\"].iloc[0]\n",
    "print(f\"Best policy is policy number: {best_policy}\\n\"\n",
    "      f\"This policy has the following values: \\n\"\n",
    "      f\"c1: {best_c1}\\n\"\n",
    "      f\"c2: {best_c2}\\n\"\n",
    "      f\"r1: {best_r1}\\n\"\n",
    "      f\"r2: {best_r2}\\n\"\n",
    "      f\"w1: {best_w1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an understanding of which solutions have decent robustness using 2 different robustness metrics. A related but different question is to assess the uncertain conditions under which we get poor performance. For this, we can use scenario discovery. Since we want to identify the uncertainties only, we can remove the policy column from the experiments DataFrame. \n",
    "\n",
    "**Perform Scenario Discovery, focussed on understanding the conditions under which utility is lower than 0.25**\n",
    "\n",
    "from the trade off curve between coverage, density and number of restricted dimensions, select a point which balances them. Next, using the `yi` attribute, select from the experiments data frame the rows which are within the box as well as the outcomes associated with these experiments.Save these results. They are the starting point for the next assignment. In pseudo code:\n",
    "\n",
    "```python\n",
    "from ema_workbench import save_results\n",
    "\n",
    "selected_experiments = experiments.iloc[box.yi]\n",
    "selected_outcomes = {k:v[box.yi] for k,v in outcomes.items()}\n",
    "\n",
    "save_results((selected_experiments, selected_outcomes), './results/selected_results.tar.gz')\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results_w_policy\n",
    "y = outcomes_w_policy['utility'] < 0.25\n",
    "prim_alg = prim.Prim(x, y, threshold=0.8)\n",
    "box1 = prim_alg.find_box()\n",
    "box1.show_tradeoff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_experiments = results_w_policy.iloc[box1.yi]\n",
    "selected_outcomes = {k : v[box1.yi] for k, v in results_w_policy.items()}\n",
    "\n",
    "save_results((selected_experiments, selected_outcomes), './results/selected_results.tar.gz')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
