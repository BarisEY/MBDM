{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPA1361 - Model-Based Decision Making\n",
    "# Week 3 - Sensitivity analysis\n",
    "\n",
    "This exercise uses the same predator-prey model we used for the multi-model exercise, focusing on the Python version. As with the other exercise, define a model object for the function below, with the uncertainty ranges provided:\n",
    "\n",
    "|Parameter\t|Range or value\t        |\n",
    "|-----------|--------------:|\n",
    "|prey_birth_rate    \t|0.015 – 0.035\t|\n",
    "|predation_rate|0.0005 – 0.003 \t|\n",
    "|predator_efficiency     \t|0.001 – 0.004\t    |\n",
    "|predator_loss_rate\t    |0.04 – 0.08\t    |\n",
    "\n",
    "* Sensitivity analysis often focuses on the final values of an outcome at the end of the simulation. However, we can also look at metrics that give us additional information about the behavior of the model over time. Using [the statsmodel library](https://www.statsmodels.org/stable/index.html) and an appropriate sampling design, fit a linear regression model for each of the following indicators. What can we conclude about the behavior of the model, and about the importance of the different inputs?\n",
    "\n",
    "  * The final values of the _prey_ outcome\n",
    "  * The mean values of the _prey_ outcome over time, within each experiment\n",
    "  * The standard deviations of the _prey_ outcome over time, within each experiment\n",
    "  \n",
    "\n",
    "* Use the Sobol sampling functionality included in the Workbench to perform experiments with a sample size of N=50, then analyze the results with SALib for the same three indicators. This requires specifying the keyword argument `'uncertainty_sampling'` of perform_experiments. Note that when using Sobol sampling, the meaning of the keyword argument `scenarios` changes a bit. In order to properly estimate Sobol scores as well as interaction effects, you require N * (2D+2) scenarios, where D is the number of uncertain parameters, and N is the value for scenarios passed to `perform_experiments`. Repeat the analysis for larger sample sizes, with N=250 and N=1000. How can we interpret the first-order and total indices? Are these sample sizes sufficient for a stable estimation of the indices? You'll need to use the [get_SALib_problem](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/em_framework/salib_samplers.html) function to convert your Workbench experiments to a problem definition that you can pass to the SALib analysis function. \n",
    "\n",
    "* *hint*: sobol is a deterministic sequence of quasi random numbers. Thus, you can run with N=1000 and simply slice for 1:50 and 1:250.\n",
    "\n",
    "* Use the [Extra-Trees analysis](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/analysis/feature_scoring.html) included in the Workbench to approximate the Sobol total indices, with a suitable sampling design. As a starting point, use an ensemble of 100 trees and a max_features parameter of 0.6, and set the analysis to regression mode. Are the estimated importances stable relative to the sample size and the analysis parameters? How do the results compare to the Sobol indices? For more details on this analysis see [Jaxa-Rozen & Kwakkel (2018)](https://www.sciencedirect.com/science/article/pii/S1364815217311581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ema_workbench'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36c879f09a0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRealParameter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimeSeriesOutcome\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperform_experiments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mema_logging\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mema_workbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mem_framework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSOBOL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMORRIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ema_workbench'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ema_workbench\n",
    "from ema_workbench import (Model, RealParameter, TimeSeriesOutcome, perform_experiments, ema_logging)\n",
    "\n",
    "from ema_workbench.em_framework.evaluators import LHS, SOBOL, MORRIS\n",
    "\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from ema_workbench.analysis.scenario_discovery_util import RuleInductionType\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from SALib.analyze import sobol\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from ema_workbench import (RealParameter, ScalarOutcome, Constant, Model)\n",
    "\n",
    "\n",
    "def pred_prey(prey_birth_rate=0.025, predation_rate=0.0015, predator_efficiency=0.002,\n",
    "             predator_loss_rate=0.06, initial_prey=50, initial_predators=20, dt=0.25, \n",
    "             final_time=365, reps=1):\n",
    "\n",
    "    #Initial values\n",
    "    predators = np.zeros((reps, int(final_time/dt)+1))\n",
    "    prey = np.zeros((reps, int(final_time/dt)+1))\n",
    "    sim_time = np.zeros((reps, int(final_time/dt)+1))\n",
    "    \n",
    "    for r in range(reps):\n",
    "\n",
    "        predators[r,0] = initial_predators\n",
    "        prey[r,0] = initial_prey\n",
    "\n",
    "    #Calculate the time series\n",
    "    for t in range(0, sim_time.shape[1]-1):\n",
    "\n",
    "        dx = (prey_birth_rate*prey[r,t]) - (predation_rate*prey[r,t]*predators[r,t])\n",
    "        dy = (predator_efficiency*predators[r,t]*prey[r,t]) - (predator_loss_rate*predators[r,t])\n",
    "\n",
    "        prey[r,t+1] = max(prey[r,t] + dx*dt, 0)\n",
    "        predators[r,t+1] = max(predators[r,t] + dy*dt, 0)\n",
    "        sim_time[r,t+1] = (t+1)*dt\n",
    "    \n",
    "    #Return outcomes\n",
    "    return {'TIME':sim_time,\n",
    "            'predators':predators,\n",
    "            'prey':prey}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating the Model\n",
    "The cell below initiates the model with the corresponding uncertainties, outcomes and constant in and of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import (RealParameter, ScalarOutcome, Constant, Model)\n",
    "\n",
    "#Creating a model instance\n",
    "model = Model('predator', function=pred_prey)\n",
    "\n",
    "#specify uncertainties\n",
    "model.uncertainties = [RealParameter('prey_birth_rate', 0.015, 0.035),\n",
    "                       RealParameter('predation_rate', 0.0005, 0.003),\n",
    "                       RealParameter('predator_efficiency', 0.001, 0.004),\n",
    "                       RealParameter('predator_loss_rate', 0.04, 0.08)]\n",
    "\n",
    "model.outcomes = [TimeSeriesOutcome('TIME'),\n",
    "                  TimeSeriesOutcome('predators'),\n",
    "                  TimeSeriesOutcome('prey')]\n",
    "\n",
    "model.constants = [Constant('initial_prey', 50),\n",
    "                   Constant('initial_predators', 20),\n",
    "                   Constant('dt', 0.25),\n",
    "                   Constant('final_time', 365),\n",
    "                   Constant('reps', 1)]\n",
    "\n",
    "n_scenarios_1 = 50\n",
    "n_scenarios_2 = 250\n",
    "n_scenarios_3 = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "In order to analyze all the different results, one function was created where the array which had to be analyzed was given to. One analysis array is given to the function, which will be the array that the sobol function uses to conduct the final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results, variable, analyze_array):\n",
    "    _, outcomes = results\n",
    "    problem = get_SALib_problem(model.uncertainties)\n",
    "    y = outcomes[variable]\n",
    "    sobol_indices = sobol.analyze(problem, analyze_array)\n",
    "    sobol_stats = {key: sobol_indices[key] for key in ['ST', 'ST_conf', 'S1',\n",
    "                                                       'S1_conf']}\n",
    "    sobol_stats = pd.DataFrame(sobol_stats, index=problem['names'])\n",
    "    sobol_stats.sort_values(by='ST', ascending=False)\n",
    "    s2 = pd.DataFrame(sobol_indices['S2'], index=problem['names'],\n",
    "                      columns=problem['names'])\n",
    "    s2_conf = pd.DataFrame(sobol_indices['S2_conf'], index=problem['names'],\n",
    "                           columns=problem['names'])\n",
    "\n",
    "    return sobol_stats, s2, s2_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "#Creating a model instance\n",
    "model = Model('predator', function=pred_prey)\n",
    "\n",
    "#specify uncertainties\n",
    "model.uncertainties = [RealParameter('prey_birth_rate', 0.015, 0.035),\n",
    "                       RealParameter('predation_rate', 0.0005, 0.003),\n",
    "                       RealParameter('predator_efficiency', 0.001, 0.004),\n",
    "                       RealParameter('predator_loss_rate', 0.04, 0.08)]\n",
    "\n",
    "model.outcomes = [TimeSeriesOutcome('TIME'),\n",
    "                  TimeSeriesOutcome('predators'),\n",
    "                  TimeSeriesOutcome('prey')]\n",
    "\n",
    "model.constants = [Constant('initial_prey', 50),\n",
    "                   Constant('initial_predators', 20),\n",
    "                   Constant('dt', 0.25),\n",
    "                   Constant('final_time', 365),\n",
    "                   Constant('reps', 1)]\n",
    "\n",
    "n_scenarios_1 = 50\n",
    "n_scenarios_2 = 250\n",
    "n_scenarios_3 = 1000\n",
    "\n",
    "with ema_workbench.SequentialEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios_1, uncertainty_sampling=SOBOL)\n",
    "\n",
    "# Use the Multi-processing evaluator in pycharm inside the main function, saves a lot of time.\n",
    "# with ema_workbench.MultiprocessingEvaluator(model) as evaluator:\n",
    "#     results = evaluator.perform_experiments(n_scenarios_1, uncertainty_sampling=SOBOL)\n",
    "\n",
    "_, outcomes = results\n",
    "y = outcomes['prey']\n",
    "last_values = np.zeros(shape=len(y))\n",
    "mean_values = np.zeros(shape=len(y))\n",
    "standard_dev_values = np.zeros(shape=len(y))\n",
    "for i in range(len(y)):\n",
    "    last_values[i] = y[i][-1][-1]\n",
    "    final_values[i] = y[i][-1].mean()\n",
    "    final_values[i] = y[i][-1].std()\n",
    "\n",
    "sobol_stats, s2, s2_conf = analyze_results(results, 'prey', last_values)\n",
    "print(sobol_stats)\n",
    "print(s2)\n",
    "print(s2_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol_stats, s2, s2_conf = analyze_results(results, 'prey', mean_values)\n",
    "print(sobol_stats)\n",
    "print(s2)\n",
    "print(s2_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobol_stats, s2, s2_conf = analyze_results(results, 'prey', standard_dev_values)\n",
    "print(sobol_stats)\n",
    "print(s2)\n",
    "print(s2_conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
